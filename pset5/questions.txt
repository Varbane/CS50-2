0.  It is the longest word in the English language published in a major dictionary, and it refers to a lung disease. It is 45 characters long, and in this problem set, it sets the upper bound for any word loaded into memory from a dictionary or checked by speller.c.
1.  getrusage provides statistics on resource use (including CPU time utilized) for the process that called it.
2.  Sixteen (fourteen long int members & two 'struct timeval' members, each of which contain two members of their own)
3.  Passing by value would be inefficient. 'before' and 'after' are both rusage structs that contain sixteen members. The calculate function is designed to only tabulate CPU time and thus needs only to access two of the sixteen members of each struct. If the entire struct were passed by value (instead of by reference), we would be passing a significant amount of data that isn't needed by the calculate function. Instead, by passing the address of each struct, the calculate function can simply access only the individual variables it needs to do its job.
4.  Once main opens the input text file to be spellchecked (with fopen in “read” mode), the for loop begins iterating through the document character-by-character via the fgetc function, storing/evaluating each char in a temporary variable c. Prior to this, a word “buffer” is prepared: an array capable of holding up to 46 chars (allowing for our upper word limit of 45 chars + null terminator) for each word to be checked. When the loop begins, the first chars of the file are read in until we hit an alphabetical character (signifying the start of the first word). This first alphabetical character is placed in the first slot of our word buffer array (word[0]), and the index variable is incremented to advance to the next slot of the array. As long as the subsequent characters are alphabetical or apostrophes, only this section of the loop (lines 82 and 83) will continue operating, filling our word buffer array, until the variable c holds a non-alphabetical/non-apostrophe character (a space, punctuation mark, etc.), denoting the end of a word. When this happens (assuming we didn’t hit a number or exceed our word length - see below), our index will be greater than 0, so the current pass of the for loop will follow the condition at line 107. Here, a null terminator is added to the current index position to signify the end of our word buffer array, we add a tally to a word counter, we check the spelling via the check function, calculate the time for the individual call to check (adding it to a grand total, time_check, which is the aggregate sum of the time taken by each of these calls), and tally/print a word if it is misspelled. At this point, the index is reset to 0 to prepare for the next word in the file. 

There are two checkpoints within the for loop to catch strings that are too long, or that contain numbers. The first (the if conditional branch in lines 86-94) checks every time a char is added to our current word index. If the index exceeds the length of our upper word limit (in this case, our word buffer array would have 46 alphabetical/apostrophe characters), the while loop in lines 89-92 will simply continue scanning through the rest of the string (i.e. until it hits a non alphabetical char), thereby ignoring it, and prepare for a new word by resetting index to 0. The second checkpoint (lines 97-104) is a conditional branch taken every time a numerical character is encountered. When this happens, the while loop in line 100 will continue scanning through the alphanumeric string via calls to fgetc, effectively ignoring the string, and preparing for a new word by resetting index to 0. It is also worth noting that the condition of our for loop will keep this entire process going until we reach the end of the input text file. The second condition ((c == '\'' && index > 0)) of the first ‘if’ branch (line 79) ensures that apostrophes are accepted as part of potential words to be spellchecked, provided they are positioned within or at the end of an alphabetical string.
5.  Using fgetc gives the program precise control on a character-by-character basis to determine whether the string it is currently reading in is a potential word to be spellchecked, something that is not offered by fscanf. Using fscanf alone, the program would pull in words as strings from an input file with trailing punctuation, and it would not catch numbers or punctuation marks within words. For instance, given the text input "Hello, m4y friend!" the fgetc method would only spellcheck "Hello" and "friend" - ignoring "m4y" and stripping punctuation. Given the same input, fscanf alone would pull in "Hello," "m4y" and "friend!" as strings, retaining punctuation and unnecessarily reading in alphanumerical strings.
6.  By declaring the pointers passed to check and load as const, it ensures that the objects they point to are "read only." In other words, check is prevented from dereferencing or otherwise modifying the word[] array passed to it. Likewise, load is prevented from modifying the dictionary filename it is pointing to. This step ensures that these functions are only doing what they are supposed to be doing (i.e. check and load should not be modifying any data at the end of the pointers passed to them, they should only be reading in the "pointees" to do their respective jobs) and makes the overall program tighter and less error-prone.
7.  I tried both methods for the problem set. I started with a trie: I designed each node of the trie to contain a boolean to indicate whether the word was present and an array of of 27 pointers to trie nodes (for each letter of the alphabet+apostrophe). My dictionary.c file also contained a global pointer to the root of the trie. 

While I implemented the trie correctly, It performed consistently slower then the staff implementation (it seemed to perform as well, and sometimes slightly faster, for large texts that had many misspellings, like the bible and works of Shakespeare, but overall, it was definitely slower than the staff implementation). Thus, I decided to try the hash table route to see if I could improve performance.

The final version of my code employs an hash table: an array of 40000 pointers to nodes for singly linked lists. Each node contains a string to store a word from the dictionary file, and a next pointer. The singly linked lists were designed to be added at the head, and they were unsorted. I used the djb2 hash function, which seems to be popular for ASCII strings. By experimenting with table size, I was able to improve the speed of my program significantly closer to the speeds achieved with the staff implementation. 
8.  My first implementation with the trie was consistently slower than the staff implementation (though generally on the same order of magnitude). For larger texts with many misspellings (KJV bible and Shakespeare), it generally performed as well as the staff implementation, but for smaller texts, it was anywhere from 25%-75% slower than the staff program. Generally speaking, where the staff's total time for a smaller text might be 0.06 seconds, my first working version's time was on the order of, say, 0.18 seconds.
9.  With my first first trie implementation, I attempted to cut down on unnecessary function calls (strlen(), islower()) in any of my larger loops. While it cleaned up my code, it didn't make any perceptible difference to the time performance.

Then, when I first retried with a hash table, I noticed a small improvement in speed compared to the trie version. From here, I went through extensive testing with the size of my hash table array. I created a few scripts to benchmark performance and compare runs of the program with different hash table sizes. I noticed a steady improvement in speed until I hit around 40,000 elements in my hash table array. This seemed to offer the optimal performance (presumably in the balance between indexing over a large array vs. scanning longer linked lists due to more collisions). It was at this point that I started to get numbers on the order of (though still somewhat slower than) those obtained from the staff implementation. Any array bigger than this did not seem to offer further improvements. 
10. Because the staff implementation still seems to be a bit faster, there are definitely places where the performance could be sped up (though I struggled to find them at a certain point!). I experimented briefly with sorted linked list that had pointers to both heads and tails (to enable searching from either direction), but I didn't notice a big difference. While I didn't attempt it, I think making each element of the hast table a pointer to a sorted binary search tree - instead of a singly-linked list - would definitely improve the speed of my program.